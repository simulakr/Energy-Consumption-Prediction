# -*- coding: utf-8 -*-
"""Energy Comsumption Prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16OYsV5DuHblA7rHx0l-ydijqau5VKIL7

# Importing Libraries
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import math
from xgboost import XGBRegressor
from lightgbm import LGBMRegressor
from sklearn.linear_model import LinearRegression
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.preprocessing import MinMaxScaler, LabelEncoder, StandardScaler, RobustScaler
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, cross_val_predict, cross_validate
from sklearn import model_selection
from sklearn.model_selection import TimeSeriesSplit
from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier
from sklearn.neighbors import KNeighborsRegressor
from sklearn.ensemble import BaggingRegressor
from warnings import filterwarnings
filterwarnings('ignore')
color_pal = sns.color_palette()
plt.style.use('fivethirtyeight')
pd.set_option('display.max_columns', None)
pd.set_option('display.width', 500)
import matplotlib.pylab as plt
import matplotlib.dates as mdates

"""## Examining the Dataset"""

df_ = pd.read_csv('/content/Energy_consumption_dataset.csv')
df = df_.copy()
df.head()

"""* **SquareFootage:** Numerical variable measuring the area of the building or space.

* **Occupancy:** Numerical variable representing the number of people in the area.

* **HVACUsage:** Categorical variable indicating the usage of Heating, Ventilation, and Air Conditioning systems.
"""

df.info()

df.describe().T

df.columns = [col.lower() for col in df.columns]

"""## Grapping Categorical and Numerical Variables"""

def grab_col_names(dataframe, cat_th=10, car_th=20):
    # cat_cols, cat_but_car
    cat_cols= [col for col in dataframe.columns if dataframe[col].dtypes == 'O']
    num_but_cat = [col for col in dataframe.columns if dataframe[col].dtypes != 'O' and
                   dataframe[col].nunique() < cat_th]
    cat_but_car = [col for col in dataframe.columns if dataframe[col].dtypes == 'O' and
                   dataframe[col].nunique() > car_th]
    cat_cols = cat_cols + num_but_cat
    cat_cols = [col for col in cat_cols if col not in cat_but_car]

    # num_cols
    num_cols = [col for col in dataframe.columns if dataframe[col].dtypes != 'O']
    num_cols = [col for col in num_cols if col not in num_but_cat ]

    print(f"Observations: {dataframe.shape[0]}")
    print(f"Variables: {dataframe.shape[1]}")
    print(f"cat_cols: {len(cat_cols)}")
    print(f"num_cols: {len(num_cols)}")
    print(f"cat_but_car: {len(cat_but_car)}")
    print(f"num_but_cat: {len(num_but_cat)}")
    return cat_cols, num_cols, cat_but_car

cat_cols, num_cols, cat_but_car = grab_col_names(df)

cat_cols

cat_cols = cat_cols + ['month', 'hour']
num_cols = [col for col in num_cols if col not in cat_cols]
target_col = 'energyconsumption'

num_cols

"""# Explorer Data Analysis"""

def cat_summary_multiple(dataframe, cat_cols, n_cols=3):
    colors = ['orange', 'blue', 'green', 'red', 'purple', 'cyan', 'brown']
    n_rows = math.ceil(len(cat_cols) / n_cols)
    fig, axes = plt.subplots(n_rows, n_cols, figsize=(18, n_rows * 5))
    axes = axes.ravel()

    for i, col in enumerate(cat_cols):
        sns.countplot(x=dataframe[col], ax=axes[i], color=colors[i % len(colors)])
        axes[i].set_title(col)

    for j in range(len(cat_cols), len(axes)):
        fig.delaxes(axes[j])

    plt.tight_layout()
    plt.show()

cat_summary_multiple(df, cat_cols, n_cols=2)

def num_summary(dataframe, col, plot=False,color='orange'):
    quantiles= [0.05, 0.10, 0.25, 0.40, 0.60, 0.75, 0.90, 0.95]
    print(dataframe[col].describe(quantiles).T)
    if plot:
        dataframe[col].hist(bins=20, color=color)
        plt.xlabel(col)
        plt.title(col)
        plt.show()
        print()

colors = ['orange', 'blue', 'green', 'red', 'purple', 'cyan', 'pink', 'gray', 'brown']
for i, col in enumerate(num_cols):
    num_summary(df, col, plot=True, color= colors[i % len(colors)])

"""# Analysis Of Variables By Target"""

def target_with_cat(dataframe, cat_col, target):
    print(pd.DataFrame({'Target Mean:':dataframe.groupby(cat_col)[target].mean(),
                        'Count': dataframe[cat_col].value_counts(),
                        'Ratio':dataframe[cat_col].value_counts() / len(dataframe)*100}))
    print()

for col in cat_cols:
    target_with_cat(df, col, 'energyconsumption')

# Numerical Variables and Target(energyconsumption)
f, ax = plt.subplots(figsize=[12,8])
sns.heatmap(df[num_cols].corr(), annot=True, fmt='.2f', ax=ax, cmap='magma')
ax.set_title('Corelation Matrix', fontsize=20)
plt.show()

df[num_cols].corrwith(df['energyconsumption']).sort_values(ascending=False)

"""## Outliers Problem"""

def outlier_thresholds(dataframe, col_name,  q1=0.05, q3=0.95):
    Q1 = dataframe[col_name].quantile(q1)
    Q3 = dataframe[col_name].quantile(q3)
    IQR = Q3 - Q1
    low = Q1 - 1.5*IQR
    up = Q3 + 1.5*IQR
    return low, up

def grab_outliers(dataframe, col_name, index=False):
    low, up = outlier_thresholds(dataframe, col_name)
    if dataframe[((dataframe[col_name] < low) | (dataframe[col_name] > up) )].shape[0] > 10:
      print(dataframe[((dataframe[col_name] < low) | (dataframe[col_name] > up) )].head(10))
    else:
      print(dataframe[((dataframe[col_name] < low) | (dataframe[col_name] > up) )])
    if index:
      outlier_index = dataframe[((dataframe[col_name] < low) | (dataframe[col_name] > up) )].index
      return outlier_index

number =1
for col in num_cols:
    print(number, '.', col.upper(), end='\n\n')
    grab_outliers(df, col)
    number += 1

"""## Feature Engineering"""

def seasons(month):
    if month in [12,1,2]:
        return "winter"
    elif month in[3,4,5]:
        return "spring"
    elif month in [6,7,8]:
        return "summer"
    else:
        return "autumn"

df["season"] = df["month"].apply(seasons)

num_cols

df['temperature_humidity'] = df['temperature'] * df['humidity']

"""## Encoding"""

cat_cols, num_cols, cat_but_car = grab_col_names(df)

def one_hot_encoder(dataframe, categorical_cols, drop_first=False):
    dataframe = pd.get_dummies(dataframe, columns=categorical_cols, drop_first=drop_first)
    return dataframe

df = one_hot_encoder(df, cat_cols, drop_first=True)

"""## Scaling"""

X_scaled = MinMaxScaler().fit_transform(df[num_cols])
df[num_cols] = pd.DataFrame(X_scaled, columns=df[num_cols].columns)
df.head()



"""## MODEL"""

y = df['energyconsumption']
X = df.drop(['energyconsumption'], axis=1)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=111)

# Shape Control
print("Train set shape:", X_train.shape)
print("Test set shape:", X_test.shape)
print("y_train shape:", y_train.shape)
print("y_test shape:", y_test.shape)

"""### Linear Regression"""

#Model Training
model1 = LinearRegression()
model1.fit(X_train, y_train)

y_pred_train = model1.predict(X_train)
y_pred_test = model1.predict(X_test)

train_rmse = mean_squared_error(y_train, y_pred_train)
test_rmse = mean_squared_error(y_test, y_pred_test)
train_r2 = r2_score(y_train, y_pred_train)
test_r2 = r2_score(y_test, y_pred_test)

print(f"Linear Regression Results:")
print(f"Train RMSE: {train_rmse:.4f}")
print(f"Test RMSE: {test_rmse:.4f}")
print(f"Train R²: {train_r2:.4f}")
print(f"Test R²: {test_r2:.4f}")

"""### Decision Tree Regression"""

model2 = DecisionTreeRegressor(random_state=111)
model2.fit(X_train, y_train)

y_pred_train = model2.predict(X_train)
y_pred_test = model2.predict(X_test)

train_rmse = mean_squared_error(y_train, y_pred_train)
test_rmse = mean_squared_error(y_test, y_pred_test)
train_r2 = r2_score(y_train, y_pred_train)
test_r2 = r2_score(y_test, y_pred_test)

print(f"Decision Tree Regression Results:")
print(f"Train RMSE: {train_rmse:.4f}")
print(f"Test RMSE: {test_rmse:.4f}")
print(f"Train R²: {train_r2:.4f}")
print(f"Test R²: {test_r2:.4f}")

"""### Random Forest Regression"""

model3 = RandomForestRegressor(random_state=111, n_estimators=100)
model3.fit(X_train, y_train)

y_pred_train = model3.predict(X_train)
y_pred_test = model3.predict(X_test)

train_rmse = mean_squared_error(y_train, y_pred_train)
test_rmse = mean_squared_error(y_test, y_pred_test)
train_r2 = r2_score(y_train, y_pred_train)
test_r2 = r2_score(y_test, y_pred_test)

print(f"Random Forest Regression Results:")
print(f"Train RMSE: {train_rmse:.4f}")
print(f"Test RMSE: {test_rmse:.4f}")
print(f"Train R²: {train_r2:.4f}")
print(f"Test R²: {test_r2:.4f}")

"""### Model comparison"""

# Model comparison
models = ['Linear Regression', 'Decision Tree', 'Random Forest']
test_rmse_scores = [0.0293, 0.0608, 0.0304]
test_r2_scores = [0.2718, -0.5124, 0.2446]

print("MODEL COMPARISON:")
print("=" * 50)
for i, model in enumerate(models):
    print(f"{model:20} | Test RMSE: {test_rmse_scores[i]:.4f} | Test R²: {test_r2_scores[i]:.4f}")

"""* Linear Regression is the best model.

# Hiperparametre Optimization
"""

from sklearn.linear_model import Ridge, Lasso, ElasticNet
from sklearn.model_selection import cross_val_score

print("REGULARIZED LINEAR MODELS:")
print("=" * 40)

# Ridge Regression
ridge = Ridge(alpha=1.0, random_state=111)
ridge.fit(X_train, y_train)
y_pred_ridge = ridge.predict(X_test)
ridge_rmse = mean_squared_error(y_test, y_pred_ridge)
ridge_r2 = r2_score(y_test, y_pred_ridge)
print(f"Ridge Regression:")
print(f"Test RMSE: {ridge_rmse:.4f}")
print(f"Test R²: {ridge_r2:.4f}")

"""## HO With GridSearchCV"""

from sklearn.model_selection import GridSearchCV

param_grid = {
    'alpha': [0.001, 0.01, 0.1, 1, 10, 100, 1000]
}

ridge_grid = GridSearchCV(Ridge(random_state=111),
                         param_grid,
                         cv=5,
                         scoring='neg_mean_squared_error',
                         n_jobs=-1)

ridge_grid.fit(X_train, y_train)

# Best parameters and results
print("Ridge Regression - GridSearch Results:")
print(f"Best Alpha: {ridge_grid.best_params_['alpha']}")
print(f"Best CV Score: {-ridge_grid.best_score_:.4f}")

# Test with best model
best_ridge = ridge_grid.best_estimator_
y_pred_best_ridge = best_ridge.predict(X_test)
best_ridge_rmse = mean_squared_error(y_test, y_pred_best_ridge)
best_ridge_r2 = r2_score(y_test, y_pred_best_ridge)

print(f"Best Ridge Test RMSE: {best_ridge_rmse:.4f}")
print(f"Best Ridge Test R²: {best_ridge_r2:.4f}")

# Lasso
lasso = Lasso(alpha=0.1, random_state=111)
lasso.fit(X_train, y_train)
y_pred_lasso = lasso.predict(X_test)
lasso_rmse = mean_squared_error(y_test, y_pred_lasso)
lasso_r2 = r2_score(y_test, y_pred_lasso)

print(f"\nLasso Regression (alpha=0.1):")
print(f"Test RMSE: {lasso_rmse:.4f}")
print(f"Test R²: {lasso_r2:.4f}")

non_zero_coef = np.sum(lasso.coef_ != 0)
print(f"Non-zero coefficients: {non_zero_coef}/{len(lasso.coef_)}")

lasso_param_grid = {
    'alpha': [0.0001, 0.001, 0.01, 0.1, 1, 10]
}

lasso_grid = GridSearchCV(Lasso(random_state=111),
                         lasso_param_grid,
                         cv=5,
                         scoring='neg_mean_squared_error',
                         n_jobs=-1)

lasso_grid.fit(X_train, y_train)

print("Lasso Regression - GridSearch Results:")
print(f"Best Alpha: {lasso_grid.best_params_['alpha']}")
print(f"Best CV Score: {-lasso_grid.best_score_:.4f}")

# Test with best Lasso model
best_lasso = lasso_grid.best_estimator_
y_pred_best_lasso = best_lasso.predict(X_test)
best_lasso_rmse = mean_squared_error(y_test, y_pred_best_lasso)
best_lasso_r2 = r2_score(y_test, y_pred_best_lasso)

print(f"Best Lasso Test RMSE: {best_lasso_rmse:.4f}")
print(f"Best Lasso Test R²: {best_lasso_r2:.4f}")

non_zero_coef_best = np.sum(best_lasso.coef_ != 0)
print(f"Non-zero coefficients: {non_zero_coef_best}/{len(best_lasso.coef_)}")

# Data Characteristics
print("\nDATA CHARACTERISTICS:")
print("=" * 30)
print(f"Feature names: {X_train.columns.tolist()}")
print(f"Training set shape: {X_train.shape}")
print(f"Target variable stats:")
print(f"  y_train mean: {y_train.mean():.4f}")
print(f"  y_train std: {y_train.std():.4f}")
print(f"  y_test mean: {y_test.mean():.4f}")
print(f"  y_test std: {y_test.std():.4f}")

"""# Final Model  Comparison"""

# Final model comparison
final_models = {
    'Linear Regression': {'RMSE': 0.0293, 'R2': 0.2718},
    'Decision Tree': {'RMSE': 0.0608, 'R2': -0.5124},
    'Random Forest': {'RMSE': 0.0304, 'R2': 0.2446},
    'Ridge (default)': {'RMSE': 0.0293, 'R2': 0.2720},
    'Lasso (optimized)': {'RMSE': 0.0291, 'R2': 0.2754}
}

print("FINAL MODEL COMPARISON:")
print("=" * 55)
print(f"{'Model':<20} | {'Test RMSE':<10} | {'Test R²':<8} | {'Status':<10}")
print("-" * 55)
for model, scores in final_models.items():
    best_rmse = scores['RMSE'] == min([m['RMSE'] for m in final_models.values()])
    best_r2 = scores['R2'] == max([m['R2'] for m in final_models.values()])
    status = "✓ BEST" if best_rmse and best_r2 else ""
    print(f"{model:<20} | {scores['RMSE']:<10.4f} | {scores['R2']:<8.4f} | {status:<10}")

"""# Feature Importance"""

feature_importance = pd.DataFrame({
    'feature': X_train.columns,
    'coefficient': best_lasso.coef_,
    'abs_coefficient': np.abs(best_lasso.coef_)
}).sort_values('abs_coefficient', ascending=False)

print("\nLASSO FEATURE IMPORTANCE:")
print("=" * 40)
print(feature_importance)

# Plot feature importance
plt.figure(figsize=(10, 6))
important_features = feature_importance[feature_importance['abs_coefficient'] > 0]
plt.barh(important_features['feature'], important_features['abs_coefficient'])
plt.xlabel('Absolute Coefficient Value')
plt.title('Lasso Feature Importance (Non-zero coefficients)')
plt.tight_layout()
plt.show()

